{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30a3290",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "A neural network consists of an input layer, one or more hidden layers, and an output layer. The input layer holds the features of the dataset, and the output layer produces the final predictions. Hidden layers lie in between and add complexity to the model. A network without hidden layers is equivalent to a linear model and helps in understanding the basics.\n",
    "\n",
    "![Neural Networks](imgs/nn.png)\n",
    "\n",
    "#### Linear Layer\n",
    "\n",
    "In PyTorch, the `torch.nn` module (commonly imported as `nn`) provides tools to define neural network layers concisely. A basic linear layer is created using `nn.Linear(in_features, out_features)`, where `in_features` is the number of input features and `out_features` is the size of the desired output. For example, a tensor of shape (1, 3) represents a single input with three features.\n",
    "\n",
    "When this input tensor is passed through a linear layer, the model performs a linear operation that includes learned weights and biases. Weights determine the importance of each input feature, while biases allow the model to make predictions even when input values are zero. Initially, weights and biases are assigned randomly and are optimized during training.\n",
    "\n",
    "In practice, for a dataset with features like temperature, humidity, and wind, the model can learn to assign more weight to humidity if it's a strong predictor of rain. Additionally, a positive bias might be added if the data comes from a region with a high baseline \n",
    "probability of rain. This combination of learned weights and biases allows the model to make informed predictions.\n",
    "\n",
    "![Linear Neural Network](imgs/linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35be00f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0955,  1.9813], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "input_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e77591",
   "metadata": {},
   "source": [
    "### Building Deeper Neural Networks\n",
    "\n",
    "#### Adding Hidden Layers with `nn.Sequential`\n",
    "To model more complex patterns in data, neural networks can include multiple hidden layers. These are stacked using PyTorch’s `nn.Sequential()` container, which allows layers to be executed in order. For example, a network might include three layers: the first transforms the input features, the second acts as a hidden layer, and the final one produces the output predictions.\n",
    "\n",
    "#### Matching Dimensions Between Layers\n",
    "Each layer in the sequence must be dimensionally compatible with the previous one. That means the output dimension of one layer must match the input dimension of the next. For instance, if the first layer outputs 18 values, the next must take 18 as input. A valid sequence could look like:\n",
    "- Input: 10 features → Layer 1: output 18\n",
    "- Layer 2: input 18 → output 20\n",
    "- Layer 3: input 20 → output 5 (final predictions)\n",
    "\n",
    "#### Neurons and Parameters\n",
    "Each linear layer is composed of neurons, and each neuron connects to all outputs from the previous layer—making it fully connected. A neuron in such a layer has `N + 1` parameters: `N` weights (one for each input feature) and one bias. The total number of parameters in a layer depends on the number of neurons and the size of the input.\n",
    "\n",
    "#### Understanding Model Capacity\n",
    "The number of hidden layers and neurons directly affects the model’s **capacity**—its ability to learn from complex data. For example:\n",
    "- A layer with 4 neurons and 8 inputs has: 4 × (8 weights + 1 bias) = 36 parameters.\n",
    "- A second layer with 2 neurons and 4 inputs has: 2 × (4 weights + 1 bias) = 10 parameters.\n",
    "- Total model parameters = 36 + 10 = 46.\n",
    "\n",
    "This parameter count can also be computed programmatically in PyTorch using the `.numel()` method, which returns the total number of elements in a tensor. Summing over all `.parameters()` in the model yields the total number of learnable parameters.\n",
    "\n",
    "#### Balancing Complexity and Efficiency\n",
    "While adding layers increases a model’s ability to capture intricate patterns, it also introduces risks such as overfitting and slower training. Therefore, understanding and controlling the number of parameters is crucial to achieving a good balance between model complexity and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef2757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2978]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Create a container for stacking linear layers\n",
    "model = nn.Sequential(nn.Linear(8, 4),\n",
    "                nn.Linear(4, 1)\n",
    "                )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951c84b",
   "metadata": {},
   "source": [
    "#### Counting the number of parameters\n",
    "Deep learning models are famous for having a lot of parameters. With more parameters comes more computational complexity and longer training times, and a deep learning practitioner must know how many parameters their model has.\n",
    "\n",
    "In this exercise, you'll first calculate the number of parameters manually. Then, you'll verify your result using the `.numel()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28b429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in the model is 53\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(9, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for p in model.parameters():\n",
    "  total += p.numel()\n",
    "  \n",
    "print(f\"The number of parameters in the model is {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55dda8",
   "metadata": {},
   "source": [
    "### Introducing Activation Functions\n",
    "\n",
    "#### Why Activation Functions Matter  \n",
    "Neural networks made of only linear layers are limited in the complexity of patterns they can learn. Activation functions introduce non-linearity, enabling models to capture intricate relationships between inputs and outputs. Two commonly used activation functions are **sigmoid** (for binary classification) and **softmax** (for multi-class classification). The output of the final linear layer, often called the **pre-activation output**, is passed through one of these functions to produce a meaningful prediction.\n",
    "\n",
    "\n",
    "### Sigmoid Activation for Binary Classification\n",
    "\n",
    "The **sigmoid** function maps any real-valued input to a value between 0 and 1, making it ideal for binary classification. For instance, if we input data about an animal—like number of limbs, whether it lays eggs, and presence of hair—a network with two linear layers might output a raw value like 6. Passing this through the sigmoid function converts it to a probability. If the output is above 0.5, we classify it as class 1 (e.g., mammal); otherwise, class 0.\n",
    "\n",
    "In PyTorch, this is done using `nn.Sigmoid()`, which can be added as the final layer in an `nn.Sequential()` model. A network with only linear layers followed by a sigmoid behaves like logistic regression. Adding more layers and activations extends this into a full deep learning model.\n",
    "\n",
    "\n",
    "### Softmax Activation for Multi-Class Classification\n",
    "\n",
    "For problems involving more than two classes, the **softmax** function is used. It converts a vector of raw scores into a probability distribution across multiple classes. For example, if we have three classes—bird (0), mammal (1), and reptile (2)—a network might output three pre-activation values. Softmax transforms these into values between 0 and 1 that sum to one, indicating the model’s confidence for each class. The class with the highest value is the predicted label.\n",
    "\n",
    "In PyTorch, this is implemented with `nn.Softmax(dim=-1)`, where `dim=-1` specifies that softmax should be applied across the last dimension of the input tensor. Like sigmoid, softmax is usually the final layer in classification models.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Activation functions are crucial for transforming raw model outputs into interpretable predictions. Sigmoid is suited for binary outcomes, while softmax handles multiple classes. Including these in the final layer of a network enables it to produce probabilities that guide accurate classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a80d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9168]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[2.4]])\n",
    "\n",
    "# Create a sigmoid function and apply it on input_tensor\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b52108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Compuomart\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = nn.Softmax()\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95862ad5",
   "metadata": {},
   "source": [
    "#### From regression to multi-class classification\n",
    "The models you have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.\n",
    "\n",
    "Start building a model for regression, and then tweak the model to perform a multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af9d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0160]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 10),\n",
    "  nn.Linear(10, 5),\n",
    "  nn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d18472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2748, 0.1368, 0.4420, 0.1465]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4),\n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae51a8",
   "metadata": {},
   "source": [
    "### Understanding Loss Functions in Neural Networks\n",
    "\n",
    "#### Purpose of a Loss Function  \n",
    "After generating predictions from a neural network, the next step is to assess how close those predictions are to the actual labels. This is where a **loss function** comes in—it quantifies the error between predicted outputs (`ŷ`) and true labels (`y`) by returning a single numerical value. A low loss means accurate predictions, while a high loss indicates poor performance. The training process aims to **minimize this loss**.\n",
    "\n",
    "\n",
    "#### Example: Multi-Class Classification  \n",
    "Consider a model that classifies animals into three classes: mammal (0), bird (1), or reptile (2). If the true label is 0 (e.g., a bear) and the model correctly predicts 0, the loss is low. If it predicts incorrectly, the loss is high. This feedback guides the model to improve over time.\n",
    "\n",
    "\n",
    "#### One-Hot Encoding for Ground Truth  \n",
    "The model’s predictions (`ŷ`) are typically raw scores (logits) from the last layer before softmax. To compare these with the true labels, we often convert the integer class labels into **one-hot encoded** vectors. For example:\n",
    "- Label 0 → `[1, 0, 0]`\n",
    "- Label 1 → `[0, 1, 0]`\n",
    "- Label 2 → `[0, 0, 1]`\n",
    "\n",
    "This ensures that predictions and ground truths have compatible shapes for loss computation.\n",
    "\n",
    "\n",
    "#### Using `torch.nn.functional` for One-Hot Encoding  \n",
    "Instead of manually creating one-hot vectors, we can use PyTorch’s `torch.nn.functional` module (imported as `F`) to transform integer labels automatically. This makes the code more efficient and readable.\n",
    "\n",
    "\n",
    "#### Cross-Entropy Loss in PyTorch  \n",
    "For multi-class classification, **cross-entropy loss** is the most commonly used loss function. In PyTorch:\n",
    "- Define the loss using `F.cross_entropy()` or `nn.CrossEntropyLoss()`\n",
    "- Pass the raw output scores (`yhat`) and the true labels (`y`, not one-hot encoded for `CrossEntropyLoss`)\n",
    "- The function returns the loss as a single float value\n",
    "\n",
    "Note: If using one-hot encoded labels, other formulations like negative log likelihood loss might be used after applying `log_softmax`.\n",
    "\n",
    "\n",
    "#### Summary  \n",
    "A loss function is critical in training neural networks—it converts the model’s prediction error into a single number to be minimized. In classification, this often involves comparing logits against true class labels (converted to one-hot if needed), using cross-entropy loss. Reducing this value through training helps the model improve its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03b1928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot vector using NumPy: [0 1 0]\n",
      "One-hot vector using PyTorch: tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes=3)\n",
    "\n",
    "print(\"One-hot vector using NumPy:\", one_hot_numpy)\n",
    "print(\"One-hot vector using PyTorch:\", one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25837a5",
   "metadata": {},
   "source": [
    "#### Calculating cross entropy loss\n",
    "Cross-entropy loss is a widely used method to measure classification loss. In this exercise, you’ll calculate cross-entropy loss in PyTorch using:\n",
    "\n",
    "- `y`: the ground truth label.\n",
    "- `scores`: a vector of predictions before softmax.\n",
    "Loss functions help neural networks learn by measuring prediction errors. Create a one-hot encoded vector for `y`, define the cross-entropy loss function, and compute the loss using scores and the encoded label. The result will be a single float representing the sample's loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3fb3364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=4)\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238896c",
   "metadata": {},
   "source": [
    "#### Accessing the model parameters\n",
    "A PyTorch model created with the `nn.Sequential()` is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd24418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[ 0.0556,  0.1620, -0.1934,  0.1200,  0.1007, -0.2213, -0.1016,  0.0295,\n",
      "         -0.2418,  0.1048, -0.1359,  0.0166, -0.1304,  0.2054,  0.0322, -0.2187],\n",
      "        [-0.2208, -0.1768,  0.2165,  0.1019, -0.0408,  0.2042,  0.0008,  0.2312,\n",
      "         -0.0573,  0.0151, -0.0055, -0.0071, -0.1824,  0.0410, -0.1710,  0.2199],\n",
      "        [-0.1586,  0.0711, -0.0112,  0.0511, -0.1203,  0.1919,  0.1554,  0.0764,\n",
      "         -0.0466,  0.0250, -0.2487,  0.2069,  0.0811,  0.1786, -0.0388,  0.0159],\n",
      "        [ 0.0548, -0.1581,  0.1943,  0.1269, -0.1036, -0.1508,  0.2448,  0.0042,\n",
      "         -0.0769,  0.0795, -0.0021, -0.0463,  0.0521, -0.2284,  0.1319, -0.0254],\n",
      "        [ 0.2497,  0.1132, -0.0689,  0.1682, -0.2266,  0.0428,  0.2372, -0.0420,\n",
      "         -0.0439, -0.0914,  0.1540, -0.0801, -0.0773, -0.0703,  0.1743,  0.1884],\n",
      "        [-0.2315,  0.2017,  0.0963,  0.0856,  0.0508, -0.2499, -0.0068,  0.2059,\n",
      "         -0.0555, -0.0276, -0.1998,  0.0241,  0.1520,  0.1452,  0.2153,  0.0285],\n",
      "        [ 0.2153,  0.1145, -0.2099, -0.0463,  0.1184,  0.1962,  0.0720,  0.0963,\n",
      "         -0.1686, -0.0158,  0.0624,  0.0563, -0.0558, -0.1384, -0.1839,  0.1827],\n",
      "        [-0.0103,  0.0778,  0.2482, -0.1894,  0.1283,  0.1427,  0.1655,  0.1474,\n",
      "          0.0107,  0.0868, -0.0862,  0.1761,  0.1326, -0.2026,  0.2328,  0.1400]],\n",
      "       requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([ 0.0228, -0.2275], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 2)\n",
    "                     )\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "print(\"Weight of the first layer:\", weight_0)\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[1].bias\n",
    "print(\"Bias of the second layer:\", bias_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f324c",
   "metadata": {},
   "source": [
    "#### Accessing the model parameters\n",
    "A PyTorch model created with the `nn.Sequential()` is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network.\n",
    "\n",
    "#### Updating the weights manually\n",
    "Now that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. While PyTorch automates this, practicing it manually helps you build intuition for how models learn and adjust. This understanding will be valuable when debugging or fine-tuning neural networks.\n",
    "\n",
    "A neural network of three layers has been created and stored as the model variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, lr, has been chosen to scale the gradients when performing the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e453c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 2),\n",
    "                      nn.Linear(2, 1)\n",
    "                     )\n",
    "\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0]])\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(output, torch.tensor([0]))\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Set the learning rate\n",
    "lr = 0.001\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - grads0 * lr\n",
    "weight1 = weight1 - grads1 * lr\n",
    "weight2 = weight2 - grads2 * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbdd25",
   "metadata": {},
   "source": [
    "#### Using the PyTorch optimizer\n",
    "Earlier, you manually updated the weight of a network, gaining insight into how training works behind the scenes. However, this method isn’t scalable for deep networks with many layers.\n",
    "\n",
    "Thankfully, PyTorch provides the SGD optimizer, which automates this process efficiently in just a few lines of code. Now, you’ll complete the training loop by updating the weights using a PyTorch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4566dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 2),\n",
    "                      nn.Linear(2, 1)\n",
    "                     )\n",
    "\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0]])\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(output, torch.tensor([0]))\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1659391",
   "metadata": {},
   "source": [
    "#### Using TensorDataset\n",
    "Structuring your data into a dataset is one of the first steps in training a PyTorch neural network. `TensorDataset` simplifies this by converting NumPy arrays into a format PyTorch can use.\n",
    "\n",
    "In this exercise, you'll create a `TensorDataset` using the preloaded `animals` dataset and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "226a677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: tensor([0, 1, 1, 0, 0, 2, 1])\n",
      "Label sample: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"animal_name\": [\"sparrow\", \"eagle\", \"cat\", \"dog\", \"lizard\"],\n",
    "    \"hair\": [0, 0, 1, 1, 0],\n",
    "    \"feathers\": [1, 1, 0, 0, 0],\n",
    "    \"eggs\": [1, 1, 0, 0, 1],\n",
    "    \"milk\": [0, 0, 1, 1, 0],\n",
    "    \"predator\": [0, 1, 1, 0, 1],\n",
    "    \"legs\": [2, 2, 4, 4, 4],\n",
    "    \"tail\": [1, 1, 1, 1, 1],\n",
    "    \"type\": [0, 0, 1, 1, 2]\n",
    "}\n",
    "\n",
    "animals = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = animals.iloc[:, 1:-1].to_numpy()  \n",
    "y = animals.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "\n",
    "# Print the first sample\n",
    "input_sample, label_sample = dataset[0]\n",
    "print('Input sample:', input_sample)\n",
    "print('Label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d3155",
   "metadata": {},
   "source": [
    "#### Using DataLoader\n",
    "The `DataLoader` class is essential for efficiently handling large datasets. It speeds up training, optimizes memory usage, and stabilizes gradient updates, making deep learning models more effective.\n",
    "\n",
    "Now, you'll create a PyTorch `DataLoader` using the `dataset` from the previous exercise and see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2fa1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs: tensor([[1, 0, 0, 1, 0, 4, 1],\n",
      "        [1, 0, 0, 1, 1, 4, 1]])\n",
      "batch_labels: tensor([1, 1])\n",
      "batch_inputs: tensor([[0, 1, 1, 0, 0, 2, 1],\n",
      "        [0, 1, 1, 0, 1, 2, 1]])\n",
      "batch_labels: tensor([0, 0])\n",
      "batch_inputs: tensor([[0, 0, 1, 0, 1, 4, 1]])\n",
      "batch_labels: tensor([2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print('batch_inputs:', batch_inputs)\n",
    "    print('batch_labels:', batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28758b56",
   "metadata": {},
   "source": [
    "### Building and Running a Training Loop in PyTorch\n",
    "\n",
    "To train a deep learning model in PyTorch, we need four main components:  \n",
    "1. A defined model  \n",
    "2. A loss function  \n",
    "3. A dataset  \n",
    "4. An optimizer  \n",
    "\n",
    "Together, these components enable a **training loop**, where the model learns by repeatedly adjusting its parameters to minimize prediction error.\n",
    "\n",
    "\n",
    "### Dataset and Problem Type  \n",
    "In this example, we work with a **regression task**: predicting normalized data scientist salaries using categorical features. Since the target is continuous, we use a **linear layer** as the model's output (not softmax or sigmoid) and **mean squared error (MSE)** as the loss function. MSE measures the average squared difference between predicted and actual values.\n",
    "\n",
    "\n",
    "### Preparing Data for Training  \n",
    "- Data (features and targets) is stored as NumPy arrays and converted to float tensors.\n",
    "- We wrap them in `TensorDataset` and use `DataLoader` to create batches (e.g., batch size = 4).\n",
    "- A simple model is defined with input features and one output neuron.\n",
    "- We use `nn.MSELoss()` as the criterion and `torch.optim.SGD` (or another optimizer) with a default learning rate of `0.001`.\n",
    "\n",
    "\n",
    "### Training Loop Structure  \n",
    "The training loop iterates over the dataset for multiple **epochs**. In each epoch:\n",
    "1. Loop over batches from the `DataLoader`.\n",
    "2. Clear existing gradients with `optimizer.zero_grad()`.\n",
    "3. Perform a forward pass to get predictions.\n",
    "4. Compute the loss using predicted and true values.\n",
    "5. Call `.backward()` to compute gradients.\n",
    "6. Update model parameters using `optimizer.step()`.\n",
    "\n",
    "This loop allows the model to learn from data by gradually reducing the loss value over successive epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b418f",
   "metadata": {},
   "source": [
    "#### Using the MSELoss\n",
    "For regression problems, you often use Mean Squared Error (MSE) as a loss function instead of cross-entropy. MSE calculates the squared difference between predicted values (`y_pred`) and actual values (`y`). Now, you'll compute MSE loss using both NumPy and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5465c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (NumPy): 0.375\n",
      "MSE (PyTorch): tensor(0.3750, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([3, 5.0, 2.5, 7.0])  \n",
    "y = np.array([3.0, 4.5, 2.0, 8.0])     \n",
    "\n",
    "# Calculate MSE using NumPy\n",
    "mse_numpy = np.mean((y-y_pred)**2)\n",
    "\n",
    "# Create the MSELoss function in PyTorch\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate MSE using PyTorch\n",
    "mse_pytorch = criterion(torch.tensor(y), torch.tensor(y_pred))\n",
    "\n",
    "print(\"MSE (NumPy):\", mse_numpy)\n",
    "print(\"MSE (PyTorch):\", mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ebc7a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc56242b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Run a forward pass\u001b[39;00m\n\u001b[0;32m     12\u001b[0m feature, target \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 13\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model(feature)    \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(prediction, target)    \n",
      "File \u001b[1;32mc:\\Users\\Compuomart\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Compuomart\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Compuomart\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Compuomart\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Compuomart\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Compuomart\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "# Set the learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Compute the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97250c89",
   "metadata": {},
   "source": [
    "### Understanding ReLU and Its Importance in Neural Networks\n",
    "\n",
    "#### Limitations of Sigmoid and Softmax  \n",
    "While **sigmoid** and **softmax** are useful activation functions—commonly used in output layers—they have limitations when used in hidden layers. Both functions produce outputs between 0 and 1, and their gradients become very small for extreme input values. This **saturation** leads to the **vanishing gradients problem**, where gradients shrink during backpropagation, preventing effective weight updates. As a result, these functions are unsuitable for deep networks' internal layers.\n",
    "\n",
    "### ReLU: Rectified Linear Unit  \n",
    "The **ReLU (Rectified Linear Unit)** activation function addresses the vanishing gradient issue by outputting the input directly if it's positive, and zero if it's negative. This unbounded behavior for positive values ensures that gradients remain large enough for effective learning. ReLU is widely used in hidden layers and can be applied in PyTorch using `torch.nn.ReLU()`. It's considered a robust default activation for many deep learning tasks.\n",
    "\n",
    "\n",
    "### Leaky ReLU: Handling Negative Inputs  \n",
    "To improve upon ReLU, **Leaky ReLU** allows a small, non-zero gradient for negative inputs. Instead of outputting zero for all negative values, it multiplies them by a small constant (e.g., 0.01). This prevents neurons from dying (i.e., becoming inactive) during training. In PyTorch, `torch.nn.LeakyReLU()` includes a `negative_slope` parameter to control this coefficient.\n",
    "\n",
    "### Summary  \n",
    "While sigmoid and softmax are useful for classification outputs, **ReLU and its variants like Leaky ReLU** are better suited for hidden layers in deep networks. They maintain stronger gradients and support more effective learning across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c35f4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU applied to positive value: tensor(2.)\n",
      "ReLU applied to negative value: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "x_pos = torch.tensor(2.0)\n",
    "x_neg = torch.tensor(-3.0)\n",
    "\n",
    "# Apply the ReLU function to the tensors\n",
    "output_pos = relu_pytorch(x_pos)\n",
    "output_neg = relu_pytorch(x_neg)\n",
    "\n",
    "print(\"ReLU applied to positive value:\", output_pos)\n",
    "print(\"ReLU applied to negative value:\", output_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31bb19c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1000)\n"
     ]
    }
   ],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope=0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608d22c",
   "metadata": {},
   "source": [
    "### Learning Rate and Momentum in Optimization\n",
    "\n",
    "#### Role of SGD in Training  \n",
    "Training a neural network involves minimizing a **loss function** by updating model parameters using an optimizer. The commonly used **Stochastic Gradient Descent (SGD)** algorithm updates weights based on the gradient of the loss and is influenced by two important hyperparameters: **learning rate** and **momentum**.\n",
    "\n",
    "\n",
    "### Learning Rate  \n",
    "The **learning rate** determines the size of each update step:\n",
    "- An **optimal learning rate** allows the optimizer to converge steadily toward the minimum.\n",
    "- A **small learning rate** slows convergence, requiring more steps to reach the minimum.\n",
    "- A **high learning rate** can cause the optimizer to overshoot and oscillate, failing to find the minimum.\n",
    "\n",
    "The step size naturally decreases as the gradient becomes smaller near the minimum, since updates are proportional to the gradient value.\n",
    "\n",
    "\n",
    "### Momentum  \n",
    "Loss functions in deep learning are typically **non-convex**, meaning they contain many local minima. Momentum helps the optimizer escape these local minima by maintaining movement in the direction of consistent gradients:\n",
    "- **Without momentum**, the optimizer can get stuck in a shallow local dip.\n",
    "- **With momentum** (e.g., 0.9), the optimizer can overcome these dips and reach better minima by building inertia.\n",
    "\n",
    "\n",
    "### Summary  \n",
    "- **Learning Rate** controls how fast the model learns; typical values range from **0.0001 to 0.01**.\n",
    "- **Momentum** adds stability and helps escape local minima; typical values range from **0.85 to 0.99**.\n",
    "Together, these hyperparameters play a crucial role in ensuring efficient and effective training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
